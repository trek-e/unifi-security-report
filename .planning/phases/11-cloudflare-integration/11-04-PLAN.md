---
phase: 11-cloudflare-integration
plan: 04
type: execute
wave: 3
depends_on: [11-02, 11-03]
files_modified:
  - tests/test_cloudflare.py
  - src/unifi_scanner/reports/generator.py
autonomous: true

must_haves:
  truths:
    - "Cloudflare models have unit tests"
    - "CloudflareIntegration Protocol compliance is tested"
    - "Template rendering is tested with sample data"
    - "Report generator passes integration results to template via 'integrations' context key"
  artifacts:
    - path: "tests/test_cloudflare.py"
      provides: "Cloudflare integration tests"
      min_lines: 100
    - path: "src/unifi_scanner/reports/generator.py"
      provides: "Integration wiring in report generation"
      contains: "integrations"
  key_links:
    - from: "src/unifi_scanner/reports/generator.py"
      to: "IntegrationRunner"
      via: "Import and instantiation"
      pattern: "from unifi_scanner\\.integrations import.*IntegrationRunner"
    - from: "src/unifi_scanner/reports/generator.py"
      to: "IntegrationResults"
      via: "_build_context integrations parameter"
      pattern: "def _build_context.*integrations"
    - from: "src/unifi_scanner/reports/generator.py"
      to: "template context"
      via: "integrations key in context dict"
      pattern: '"integrations".*integrations'
---

<objective>
Create tests for Cloudflare integration and wire integration results into report generator.

Purpose: Ensures Cloudflare integration works correctly and integration data flows to templates during report generation.

Output: Comprehensive test coverage for Cloudflare module. Report generator runs integrations and passes results to templates.
</objective>

<execution_context>
@/Users/trekkie/.claude/get-shit-done/workflows/execute-plan.md
@/Users/trekkie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/11-cloudflare-integration/11-01-SUMMARY.md
@.planning/phases/11-cloudflare-integration/11-02-SUMMARY.md
@.planning/phases/11-cloudflare-integration/11-03-SUMMARY.md
@src/unifi_scanner/reports/generator.py
@src/unifi_scanner/integrations/runner.py
@src/unifi_scanner/integrations/base.py
@tests/test_integrations.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Cloudflare integration tests</name>
  <files>
    tests/test_cloudflare.py
  </files>
  <action>
Create comprehensive tests following the pattern from test_integrations.py:

```python
"""Tests for Cloudflare integration.

Tests models, client mocking, integration Protocol compliance, and template rendering.
"""

from __future__ import annotations

from datetime import datetime, timezone
from typing import TYPE_CHECKING
from unittest.mock import AsyncMock, MagicMock, patch

import pytest

from unifi_scanner.integrations.base import IntegrationResult
from unifi_scanner.integrations.cloudflare.models import (
    CloudflareData,
    DNSAnalytics,
    TunnelStatus,
    WAFEvent,
)

if TYPE_CHECKING:
    pass


# ============================================================================
# Model Tests
# ============================================================================


class TestWAFEvent:
    """Tests for WAFEvent model."""

    def test_minimal_waf_event(self):
        """WAFEvent with only required fields."""
        event = WAFEvent(
            timestamp=datetime.now(timezone.utc),
            action="block",
            source_ip="1.2.3.4",
            rule_source="waf",
        )
        assert event.source_ip == "1.2.3.4"
        assert event.action == "block"
        assert event.country == ""  # default

    def test_full_waf_event(self):
        """WAFEvent with all fields populated."""
        event = WAFEvent(
            timestamp=datetime.now(timezone.utc),
            action="block",
            source_ip="1.2.3.4",
            country="CN",
            path="/admin",
            host="example.com",
            rule_source="firewallrules",
            rule_id="abc123",
            user_agent="bad-bot/1.0",
            zone_name="example.com",
        )
        assert event.country == "CN"
        assert event.zone_name == "example.com"


class TestDNSAnalytics:
    """Tests for DNSAnalytics model."""

    def test_empty_dns_analytics(self):
        """DNSAnalytics with default values."""
        analytics = DNSAnalytics()
        assert analytics.total_queries == 0
        assert analytics.blocked_queries == 0
        assert analytics.top_blocked_domains == []

    def test_populated_dns_analytics(self):
        """DNSAnalytics with data."""
        analytics = DNSAnalytics(
            total_queries=1000,
            blocked_queries=50,
            allowed_queries=950,
            top_blocked_domains=["malware.com", "ads.example.com"],
        )
        assert analytics.total_queries == 1000
        assert len(analytics.top_blocked_domains) == 2


class TestTunnelStatus:
    """Tests for TunnelStatus model."""

    def test_tunnel_status(self):
        """TunnelStatus model."""
        tunnel = TunnelStatus(
            name="prod-tunnel",
            tunnel_id="abc123",
            status="healthy",
        )
        assert tunnel.name == "prod-tunnel"
        assert tunnel.status == "healthy"


class TestCloudflareData:
    """Tests for CloudflareData aggregation model."""

    def test_empty_cloudflare_data(self):
        """CloudflareData with no data."""
        data = CloudflareData()
        assert not data.has_waf_events
        assert not data.has_dns_analytics
        assert not data.has_tunnels

    def test_cloudflare_data_with_waf_events(self):
        """CloudflareData with WAF events."""
        events = [
            WAFEvent(
                timestamp=datetime.now(timezone.utc),
                action="block",
                source_ip="1.2.3.4",
                rule_source="waf",
            ),
            WAFEvent(
                timestamp=datetime.now(timezone.utc),
                action="block",
                source_ip="1.2.3.4",
                rule_source="waf",
            ),
            WAFEvent(
                timestamp=datetime.now(timezone.utc),
                action="block",
                source_ip="5.6.7.8",
                rule_source="firewallrules",
            ),
        ]
        data = CloudflareData(waf_events=events)

        assert data.has_waf_events
        assert len(data.waf_events) == 3

        # Test grouping
        by_source = data.get_waf_by_source()
        assert "waf" in by_source
        assert "firewallrules" in by_source
        assert len(by_source["waf"]) == 2

        # Test top IPs
        top_ips = data.get_top_blocked_ips()
        assert top_ips[0] == ("1.2.3.4", 2)
        assert top_ips[1] == ("5.6.7.8", 1)

    def test_cloudflare_data_down_tunnels(self):
        """CloudflareData filters down tunnels."""
        tunnels = [
            TunnelStatus(name="prod", tunnel_id="1", status="healthy"),
            TunnelStatus(name="dev", tunnel_id="2", status="down"),
            TunnelStatus(name="staging", tunnel_id="3", status="degraded"),
        ]
        data = CloudflareData(tunnels=tunnels)

        assert data.has_tunnels
        assert len(data.down_tunnels) == 2
        assert data.down_tunnels[0].name == "dev"


# ============================================================================
# Integration Protocol Tests
# ============================================================================


class TestCloudflareIntegration:
    """Tests for CloudflareIntegration Protocol compliance."""

    @pytest.fixture
    def mock_settings(self):
        """Create mock settings."""
        settings = MagicMock()
        settings.cloudflare_api_token = None
        settings.cloudflare_account_id = None
        settings.initial_lookback_hours = 24
        return settings

    def test_not_configured_without_token(self, mock_settings):
        """is_configured() returns False without token."""
        from unifi_scanner.integrations.cloudflare.integration import CloudflareIntegration

        integration = CloudflareIntegration(mock_settings)
        assert integration.name == "cloudflare"
        assert not integration.is_configured()

    def test_configured_with_token(self, mock_settings):
        """is_configured() returns True with token."""
        from unifi_scanner.integrations.cloudflare.integration import CloudflareIntegration

        mock_settings.cloudflare_api_token = "test_token"
        integration = CloudflareIntegration(mock_settings)
        assert integration.is_configured()

    def test_validate_config_warns_missing_account_id(self, mock_settings):
        """validate_config() warns when account_id missing."""
        from unifi_scanner.integrations.cloudflare.integration import CloudflareIntegration

        mock_settings.cloudflare_api_token = "test_token"
        integration = CloudflareIntegration(mock_settings)

        warning = integration.validate_config()
        assert warning is not None
        assert "CLOUDFLARE_ACCOUNT_ID" in warning

    def test_validate_config_no_warning_when_complete(self, mock_settings):
        """validate_config() returns None when fully configured."""
        from unifi_scanner.integrations.cloudflare.integration import CloudflareIntegration

        mock_settings.cloudflare_api_token = "test_token"
        mock_settings.cloudflare_account_id = "test_account"
        integration = CloudflareIntegration(mock_settings)

        warning = integration.validate_config()
        assert warning is None

    @pytest.mark.asyncio
    async def test_fetch_returns_error_when_not_configured(self, mock_settings):
        """fetch() returns error when not configured."""
        from unifi_scanner.integrations.cloudflare.integration import CloudflareIntegration

        integration = CloudflareIntegration(mock_settings)
        result = await integration.fetch()

        assert not result.success
        assert result.error == "Not configured"

    @pytest.mark.asyncio
    async def test_fetch_calls_client(self, mock_settings):
        """fetch() calls CloudflareClient.fetch_all()."""
        from unifi_scanner.integrations.cloudflare.integration import CloudflareIntegration

        mock_settings.cloudflare_api_token = "test_token"
        mock_settings.cloudflare_account_id = "test_account"

        integration = CloudflareIntegration(mock_settings)

        # Mock the client
        mock_data = CloudflareData(
            waf_events=[
                WAFEvent(
                    timestamp=datetime.now(timezone.utc),
                    action="block",
                    source_ip="1.2.3.4",
                    rule_source="waf",
                )
            ],
            zones_queried=["example.com"],
        )

        with patch(
            "unifi_scanner.integrations.cloudflare.integration.CloudflareClient"
        ) as MockClient:
            mock_instance = AsyncMock()
            mock_instance.fetch_all = AsyncMock(return_value=mock_data)
            MockClient.return_value = mock_instance

            result = await integration.fetch()

            assert result.success
            assert result.data is not None
            assert result.data["waf_count"] == 1
            assert result.data["has_waf_events"] is True


# ============================================================================
# Template Rendering Tests
# ============================================================================


class TestCloudflareTemplateRendering:
    """Tests for cloudflare_section.html template."""

    @pytest.fixture
    def jinja_env(self):
        """Create Jinja environment."""
        from jinja2 import Environment, FileSystemLoader

        return Environment(
            loader=FileSystemLoader("src/unifi_scanner/reports/templates")
        )

    def test_template_renders_with_waf_events(self, jinja_env):
        """Template renders WAF events section."""
        template = jinja_env.get_template("cloudflare_section.html")

        cloudflare = {
            "has_waf_events": True,
            "waf_count": 2,
            "waf_by_source": {
                "waf": [
                    {"source_ip": "1.2.3.4", "country": "CN", "host": "example.com", "path": "/admin"},
                ],
            },
            "top_blocked_ips": [("1.2.3.4", 2)],
            "has_dns_analytics": False,
            "dns_analytics": None,
            "has_tunnels": False,
            "tunnels": [],
            "has_down_tunnels": False,
            "down_tunnels": [],
            "zones_queried": ["example.com"],
        }

        html = template.render(cloudflare=cloudflare)

        assert "Cloudflare Security" in html
        assert "WAF Blocks" in html
        assert "1.2.3.4" in html
        assert "example.com" in html

    def test_template_renders_dns_analytics(self, jinja_env):
        """Template renders DNS analytics section."""
        template = jinja_env.get_template("cloudflare_section.html")

        cloudflare = {
            "has_waf_events": False,
            "waf_count": 0,
            "waf_by_source": {},
            "top_blocked_ips": [],
            "has_dns_analytics": True,
            "dns_analytics": {
                "total_queries": 1000,
                "blocked_queries": 50,
                "allowed_queries": 950,
                "top_blocked_domains": ["malware.com"],
            },
            "has_tunnels": False,
            "tunnels": [],
            "has_down_tunnels": False,
            "down_tunnels": [],
            "zones_queried": [],
        }

        html = template.render(cloudflare=cloudflare)

        assert "DNS Analytics" in html
        assert "1000" in html  # total
        assert "50" in html  # blocked
        assert "malware.com" in html

    def test_template_renders_tunnels(self, jinja_env):
        """Template renders tunnel status section."""
        template = jinja_env.get_template("cloudflare_section.html")

        cloudflare = {
            "has_waf_events": False,
            "waf_count": 0,
            "waf_by_source": {},
            "top_blocked_ips": [],
            "has_dns_analytics": False,
            "dns_analytics": None,
            "has_tunnels": True,
            "tunnels": [
                {"name": "prod-tunnel", "status": "healthy"},
                {"name": "dev-tunnel", "status": "down"},
            ],
            "has_down_tunnels": True,
            "down_tunnels": [{"name": "dev-tunnel", "status": "down"}],
            "zones_queried": [],
        }

        html = template.render(cloudflare=cloudflare)

        assert "Tunnel Status" in html
        assert "prod-tunnel" in html
        assert "HEALTHY" in html
        assert "DOWN" in html
        assert "Tunnel Issues" in html  # Warning for down tunnel

    def test_template_skips_when_no_data(self, jinja_env):
        """Template renders nothing when no Cloudflare data."""
        template = jinja_env.get_template("cloudflare_section.html")

        # No data
        html = template.render(cloudflare=None)
        assert "Cloudflare" not in html

        # Empty data
        cloudflare = {
            "has_waf_events": False,
            "has_dns_analytics": False,
            "has_tunnels": False,
        }
        html = template.render(cloudflare=cloudflare)
        assert "Cloudflare Security" not in html
```
  </action>
  <verify>
```bash
cd /Users/trekkie/projects/unifi_scanner && python -m pytest tests/test_cloudflare.py -v --tb=short
```
  </verify>
  <done>
All Cloudflare tests pass. Models, integration Protocol, and template rendering covered.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire integration results into report generator</name>
  <files>
    src/unifi_scanner/reports/generator.py
  </files>
  <action>
Modify `src/unifi_scanner/reports/generator.py` to run integrations and pass results to templates.

**Step 1: Add imports at top of file (after existing imports)**

```python
from unifi_scanner.integrations import IntegrationRunner, IntegrationResults
```

**Step 2: Update __init__ to accept settings**

The ReportGenerator needs access to settings to pass to IntegrationRunner. Add an optional `settings` parameter:

```python
def __init__(
    self,
    display_timezone: str = "UTC",
    report_title: str = "UniFi Network Report",
    settings: Optional[Any] = None,  # Add this parameter
) -> None:
    """Initialize ReportGenerator with Jinja2 environment.

    Args:
        display_timezone: IANA timezone name for timestamp display
            (e.g., 'America/New_York'). Defaults to 'UTC'.
        report_title: Default title for generated reports.
        settings: Optional settings for integration runner. If None,
            integrations will return empty results (no integrations run).
    """
    self.report_title = report_title
    self.formatter = FindingFormatter(display_timezone=display_timezone)
    self._settings = settings  # Store settings for IntegrationRunner

    # Configure Jinja2 environment
    self.env = Environment(
        loader=PackageLoader("unifi_scanner.reports", "templates"),
        autoescape=select_autoescape(["html", "xml"]),
        trim_blocks=True,
        lstrip_blocks=True,
    )
```

**Step 3: Update _build_context signature to accept integrations parameter**

```python
def _build_context(
    self,
    report: Report,
    ips_analysis: Optional[ThreatAnalysisResult] = None,
    health_analysis: Optional[DeviceHealthResult] = None,
    integrations: Optional[IntegrationResults] = None,  # Add this parameter
) -> Dict[str, Any]:
    """Build template context from a Report.

    Groups findings by severity and prepares all data needed for
    template rendering.

    Args:
        report: Report containing findings to format
        ips_analysis: Optional IPS threat analysis results
        health_analysis: Optional device health analysis results
        integrations: Optional integration results from IntegrationRunner

    Returns:
        Dictionary with template context including 'integrations' key.
    """
    grouped = self.formatter.format_grouped_findings(report.findings)

    return {
        "report_title": self.report_title,
        "site_name": report.site_name,
        "period_start": self.formatter.format_timestamp(report.period_start),
        "period_end": self.formatter.format_timestamp(report.period_end),
        "generated_at": self.formatter.format_timestamp(report.generated_at),
        "severe_findings": grouped["severe"],
        "medium_findings": grouped["medium"],
        "low_findings": grouped["low"],
        "counts": {
            "severe_count": len(grouped["severe"]),
            "medium_count": len(grouped["medium"]),
            "low_count": len(grouped["low"]),
            "total": len(report.findings),
        },
        "ips_analysis": ips_analysis,
        "health_analysis": health_analysis,
        "integrations": integrations,  # Add integrations to context dict
    }
```

**Step 4: Update generate_html and generate_text to be async and run integrations**

Change both methods to async and call IntegrationRunner.run_all() before building context:

```python
async def generate_html(
    self,
    report: Report,
    ips_analysis: Optional[ThreatAnalysisResult] = None,
    health_analysis: Optional[DeviceHealthResult] = None,
) -> str:
    """Generate HTML report from Report model.

    HTML reports display findings organized by severity (SEVERE first,
    then MEDIUM, then LOW) with an executive summary and professional
    UniFi-inspired styling. All CSS is inline for email compatibility.

    Args:
        report: Report object containing findings to render
        ips_analysis: Optional IPS threat analysis results
        health_analysis: Optional device health analysis results

    Returns:
        Complete HTML document as string
    """
    # Run integrations if settings provided
    integrations = None
    if self._settings:
        runner = IntegrationRunner(self._settings)
        integrations = await runner.run_all()

    template = self.env.get_template("report.html")
    context = self._build_context(report, ips_analysis, health_analysis, integrations)
    return template.render(**context)

async def generate_text(
    self,
    report: Report,
    ips_analysis: Optional[ThreatAnalysisResult] = None,
    health_analysis: Optional[DeviceHealthResult] = None,
) -> str:
    """Generate plain text report from Report model.

    Plain text reports use tiered detail levels:
    - SEVERE: Full detail (title, description, occurrence, remediation)
    - MEDIUM: Summary (title, brief description, occurrence, remediation)
    - LOW: One-liner (title and occurrence count only)

    Args:
        report: Report object containing findings to render
        ips_analysis: Optional IPS threat analysis results
        health_analysis: Optional device health analysis results

    Returns:
        Plain text report as string
    """
    # Run integrations if settings provided
    integrations = None
    if self._settings:
        runner = IntegrationRunner(self._settings)
        integrations = await runner.run_all()

    template = self.env.get_template("report.txt")
    context = self._build_context(report, ips_analysis, health_analysis, integrations)
    return template.render(**context)
```

**Important notes:**
- Methods are now `async` because IntegrationRunner.run_all() is async
- Callers of generate_html/generate_text must now use `await`
- If settings is None, integrations is None (backward compatible - no integrations run)
- IntegrationRunner is instantiated fresh each call (stateless pattern)
  </action>
  <verify>
```bash
cd /Users/trekkie/projects/unifi_scanner && python -c "
# Verify imports
from unifi_scanner.reports.generator import ReportGenerator
from unifi_scanner.integrations import IntegrationRunner, IntegrationResults

# Verify _build_context accepts integrations parameter
import inspect
sig = inspect.signature(ReportGenerator._build_context)
params = list(sig.parameters.keys())
print(f'_build_context params: {params}')
assert 'integrations' in params, '_build_context must accept integrations param'

# Verify generate_html is async
assert inspect.iscoroutinefunction(ReportGenerator.generate_html), 'generate_html must be async'
assert inspect.iscoroutinefunction(ReportGenerator.generate_text), 'generate_text must be async'

# Verify __init__ accepts settings
sig = inspect.signature(ReportGenerator.__init__)
params = list(sig.parameters.keys())
print(f'__init__ params: {params}')
assert 'settings' in params, '__init__ must accept settings param'

# Read source to verify 'integrations' is added to context dict
import unifi_scanner.reports.generator as gen_module
source = inspect.getsource(gen_module)
assert '\"integrations\": integrations' in source or \"'integrations': integrations\" in source, 'integrations must be added to context dict'

print('All wiring verified!')
"
```
  </verify>
  <done>
Report generator imports IntegrationRunner, _build_context accepts integrations parameter, integrations added to context dict, generate_html/generate_text are async and run integrations when settings provided.
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_cloudflare.py -v` passes
2. Report generator imports IntegrationRunner and IntegrationResults
3. _build_context signature includes integrations parameter
4. Context dict includes "integrations" key
5. generate_html and generate_text are async and run IntegrationRunner.run_all()
6. `python -m pytest tests/ -x -q` passes (no regressions)
</verification>

<success_criteria>
- tests/test_cloudflare.py exists with model, integration, and template tests
- All Cloudflare tests pass
- Report generator imports IntegrationRunner
- _build_context accepts integrations param and adds to context dict
- generate_html/generate_text are async and run IntegrationRunner.run_all() when settings provided
- Template receives integration results via 'integrations' context key
- Existing tests still pass (may need updates for async generate methods)
</success_criteria>

<output>
After completion, create `.planning/phases/11-cloudflare-integration/11-04-SUMMARY.md`
</output>
