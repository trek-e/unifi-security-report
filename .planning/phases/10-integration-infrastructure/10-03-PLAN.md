---
phase: 10-integration-infrastructure
plan: 03
type: execute
wave: 2
depends_on: ["10-01"]
files_modified:
  - tests/test_integrations.py
autonomous: true

must_haves:
  truths:
    - "Tests verify Integration Protocol contract"
    - "Tests verify IntegrationRegistry filters by is_configured()"
    - "Tests verify IntegrationRunner isolates failures between integrations"
    - "Tests verify circuit breaker opens after 3 failures"
  artifacts:
    - path: "tests/test_integrations.py"
      provides: "Unit tests for integration infrastructure"
      min_lines: 100
  key_links:
    - from: "tests/test_integrations.py"
      to: "src/unifi_scanner/integrations/"
      via: "imports and tests"
      pattern: "from unifi_scanner\\.integrations import"
---

<objective>
Create comprehensive tests for the integration infrastructure.

Purpose: Verify the core requirements (INTG-01, INTG-02, INTG-03) are satisfied through automated tests. These tests ensure the infrastructure works correctly before Cloudflare/Cybersecure integrations are added.

Output: Test file covering Protocol contract, registry filtering, runner isolation, and circuit breaker behavior.
</objective>

<execution_context>
@/Users/trekkie/.claude/get-shit-done/workflows/execute-plan.md
@/Users/trekkie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/10-integration-infrastructure/10-CONTEXT.md
@.planning/phases/10-integration-infrastructure/10-RESEARCH.md
@src/unifi_scanner/integrations/base.py
@src/unifi_scanner/integrations/registry.py
@src/unifi_scanner/integrations/runner.py
@tests/test_ips_analyzer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create mock integrations and test registry</name>
  <files>
    tests/test_integrations.py
  </files>
  <action>
Create `tests/test_integrations.py` with:

1. Mock integration classes for testing:
   - `ConfiguredIntegration`: is_configured() returns True, fetch() returns success
   - `UnconfiguredIntegration`: is_configured() returns False
   - `PartiallyConfiguredIntegration`: is_configured() returns False, validate_config() returns warning
   - `FailingIntegration`: is_configured() returns True, fetch() raises exception
   - `SlowIntegration`: is_configured() returns True, fetch() sleeps for longer than timeout

2. Test Protocol contract:
   - `test_integration_result_dataclass`: verify fields
   - `test_integration_section_dataclass`: verify fields
   - `test_integration_results_has_data`: verify has_data property logic
   - `test_integration_results_get_section`: verify get_section method

3. Test IntegrationRegistry:
   - `test_registry_empty_when_no_integrations`: get_configured returns []
   - `test_registry_filters_unconfigured`: only configured integrations returned
   - `test_registry_logs_partial_config_warning`: partial config triggers warning log
   - `test_registry_get_all_returns_all`: get_all includes unconfigured

Use pytest fixtures for mock settings object.
Follow existing test patterns from `tests/test_ips_analyzer.py`.
  </action>
  <verify>
    pytest tests/test_integrations.py -v --tb=short -k "registry or result or section"
  </verify>
  <done>
    Registry and model tests pass
  </done>
</task>

<task type="auto">
  <name>Task 2: Test IntegrationRunner isolation and circuit breaker</name>
  <files>
    tests/test_integrations.py
  </files>
  <action>
Add runner tests to `tests/test_integrations.py`:

1. Test parallel execution with isolation (INTG-02):
   - `test_runner_isolates_failures`: One failing integration doesn't prevent others from succeeding
   - `test_runner_returns_all_results`: Results include both success and failure
   - `test_runner_empty_when_no_integrations`: Returns empty IntegrationResults

2. Test circuit breaker behavior (INTG-03):
   - `test_runner_timeout_returns_failure`: SlowIntegration returns timeout error
   - `test_circuit_breaker_opens_after_failures`: After 3 failures, circuit opens
   - `test_circuit_breaker_returns_circuit_open_error`: When open, returns "circuit_open" error

3. Test result conversion:
   - `test_runner_converts_to_sections`: IntegrationResult converted to IntegrationSection
   - `test_runner_sets_error_message_on_failure`: Failed integrations have error_message set

Use `pytest.mark.asyncio` for async tests.
Use `unittest.mock.patch` to mock circuit breaker state if needed.
Reset circuit breakers between tests to avoid state leakage.
  </action>
  <verify>
    pytest tests/test_integrations.py -v --tb=short
  </verify>
  <done>
    All integration infrastructure tests pass
  </done>
</task>

</tasks>

<verification>
```bash
# Run all integration tests
pytest tests/test_integrations.py -v --tb=short

# Verify coverage of key requirements
pytest tests/test_integrations.py -v --tb=short -k "isolate or circuit or timeout"

# Run full test suite to ensure nothing broke
pytest tests/ -v --tb=short

# Check test count is reasonable (should have 10+ tests)
pytest tests/test_integrations.py --collect-only | grep "test_" | wc -l
```
</verification>

<success_criteria>
1. Mock integrations created for various scenarios (configured, unconfigured, failing, slow)
2. Protocol/dataclass tests verify fields and methods
3. Registry tests verify filtering by is_configured()
4. Runner tests verify parallel execution with isolation
5. Circuit breaker tests verify opens after 3 failures
6. Timeout tests verify 30s timeout applied
7. All tests pass with pytest
8. At least 10 test cases covering core requirements
</success_criteria>

<output>
After completion, create `.planning/phases/10-integration-infrastructure/10-03-SUMMARY.md`
</output>
