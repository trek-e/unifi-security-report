---
phase: 03-analysis-engine
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - src/unifi_scanner/analysis/store.py
  - src/unifi_scanner/analysis/__init__.py
  - src/unifi_scanner/models/finding.py
  - tests/test_finding_store.py
autonomous: true

must_haves:
  truths:
    - "Findings are deduplicated by event_type and device_mac within 1-hour windows"
    - "Occurrence counts track how many times an issue happened"
    - "First and last seen timestamps are maintained accurately"
    - "Recurring issues (5+ occurrences) are flagged without severity escalation"
    - "Display format shows 'Occurred N times (first: X, last: Y)'"
  artifacts:
    - path: "src/unifi_scanner/analysis/store.py"
      provides: "FindingStore with time-window deduplication"
      exports: ["FindingStore"]
    - path: "src/unifi_scanner/models/finding.py"
      provides: "Finding model with is_recurring property"
      contains: "is_recurring"
  key_links:
    - from: "src/unifi_scanner/analysis/store.py"
      to: "src/unifi_scanner/models/finding.py"
      via: "Creates and updates Finding objects"
      pattern: "Finding\\("
    - from: "src/unifi_scanner/analysis/store.py"
      to: "src/unifi_scanner/analysis/engine.py"
      via: "Receives findings from AnalysisEngine.analyze_entry()"
      pattern: "add_or_merge"
---

<objective>
Create FindingStore with time-based deduplication and occurrence tracking.

Purpose: Raw analysis produces one finding per log entry, but users want deduplicated results. FindingStore groups repeated events (same event_type + device_mac) within 1-hour windows into single findings with occurrence counts. This matches user decision: "events within 1 hour are one incident."

Output: FindingStore class with add_or_merge logic, updated Finding model with is_recurring property, and tests for deduplication scenarios.
</objective>

<execution_context>
@/Users/trekkie/.claude/get-shit-done/workflows/execute-plan.md
@/Users/trekkie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-analysis-engine/03-CONTEXT.md
@.planning/phases/03-analysis-engine/03-RESEARCH.md

# Key existing files
@src/unifi_scanner/models/finding.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add is_recurring property and format_occurrence_summary to Finding model</name>
  <files>src/unifi_scanner/models/finding.py</files>
  <action>
Update the Finding model to add:

1. Class constant for recurring threshold (5 occurrences per Claude's discretion):
   ```python
   RECURRING_THRESHOLD: int = 5
   ```

2. Property `is_recurring` that returns True if occurrence_count >= threshold:
   ```python
   @property
   def is_recurring(self) -> bool:
       """Check if this finding should be flagged as recurring.

       A finding is recurring if it has many occurrences within
       the time window. Per user decision, this does NOT escalate
       severity - it's just a flag for awareness.

       Returns:
           True if occurrence_count >= RECURRING_THRESHOLD
       """
       return self.occurrence_count >= self.RECURRING_THRESHOLD
   ```

3. Method `format_occurrence_summary` that returns display string per user decision:
   ```python
   def format_occurrence_summary(self, time_format: str = "%I:%M %p") -> str:
       """Format occurrence count and time range for display.

       Per user decision: "Occurred 5 times (first: 2:00 PM, last: 4:30 PM)"

       Args:
           time_format: strftime format for times (default 12-hour with AM/PM)

       Returns:
           Formatted string describing occurrences
       """
       if self.occurrence_count == 1:
           return f"Occurred at {self.first_seen.strftime(time_format)}"

       summary = (
           f"Occurred {self.occurrence_count} times "
           f"(first: {self.first_seen.strftime(time_format)}, "
           f"last: {self.last_seen.strftime(time_format)})"
       )

       if self.is_recurring:
           summary = f"[Recurring Issue] {summary}"

       return summary
   ```

The complete updated class should include these additions after the existing `is_actionable` property.
  </action>
  <verify>
Run: `python -c "from unifi_scanner.models import Finding; from unifi_scanner.models.enums import Category, Severity; from datetime import datetime, timezone; f = Finding(severity=Severity.LOW, category=Category.SYSTEM, title='Test', description='Test', first_seen=datetime.now(timezone.utc), last_seen=datetime.now(timezone.utc), occurrence_count=6); print(f.is_recurring, 'Recurring' in f.format_occurrence_summary())"`
Should print: `True True`
  </verify>
  <done>
Finding model has is_recurring property (threshold 5), format_occurrence_summary() method for display, and recurring flag is shown in summary without severity escalation.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create FindingStore with time-window deduplication</name>
  <files>
src/unifi_scanner/analysis/store.py
src/unifi_scanner/analysis/__init__.py
  </files>
  <action>
1. Create `src/unifi_scanner/analysis/store.py`:

```python
"""Finding store with time-based deduplication.

Implements user decisions:
- Group by: same event_type + same device (device_mac)
- Time clustering: events within 1 hour are one incident
- Recurring flag for issues with many occurrences (no severity escalation)
"""

from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from uuid import UUID

import structlog

from unifi_scanner.models.finding import Finding

logger = structlog.get_logger(__name__)


class FindingStore:
    """Store for findings with automatic deduplication.

    Deduplicates findings by (event_type, device_mac) within a configurable
    time window. Events outside the window start a new finding.

    Usage:
        store = FindingStore()
        for finding in raw_findings:
            store.add_or_merge(finding, event_type, device_mac, timestamp)
        deduplicated = store.get_all_findings()
    """

    # User decision: 1-hour time window for clustering
    DEFAULT_CLUSTER_WINDOW = timedelta(hours=1)

    def __init__(self, cluster_window: Optional[timedelta] = None):
        """Initialize the finding store.

        Args:
            cluster_window: Time window for grouping events.
                          Defaults to 1 hour per user decision.
        """
        self._cluster_window = cluster_window or self.DEFAULT_CLUSTER_WINDOW

        # Main storage: finding_id -> Finding
        self._findings: Dict[UUID, Finding] = {}

        # Deduplication index: (event_type, device_mac) -> most recent finding
        # device_mac can be None for findings not tied to a device
        self._dedup_index: Dict[Tuple[str, Optional[str]], Finding] = {}

        # Statistics
        self._total_merged: int = 0
        self._total_new: int = 0

    @property
    def cluster_window(self) -> timedelta:
        """Get the time window used for clustering."""
        return self._cluster_window

    @property
    def stats(self) -> Dict[str, int]:
        """Get store statistics."""
        return {
            "total_findings": len(self._findings),
            "total_merged": self._total_merged,
            "total_new": self._total_new,
        }

    def add_or_merge(
        self,
        finding: Finding,
        event_type: str,
        device_mac: Optional[str],
        timestamp: datetime,
        log_id: UUID,
    ) -> Finding:
        """Add a new finding or merge into existing if within time window.

        Args:
            finding: The Finding object from analysis
            event_type: Event type for deduplication key
            device_mac: Device MAC for deduplication key (can be None)
            timestamp: Timestamp of this occurrence
            log_id: UUID of the source LogEntry

        Returns:
            The Finding object (either new or updated existing)
        """
        dedup_key = (event_type, device_mac)

        existing = self._dedup_index.get(dedup_key)

        if existing and self._within_window(existing.last_seen, timestamp):
            # Merge into existing finding
            existing.add_occurrence(log_id, timestamp)
            self._total_merged += 1

            logger.debug(
                "finding_merged",
                finding_id=str(existing.id),
                event_type=event_type,
                occurrence_count=existing.occurrence_count,
            )
            return existing

        # Create new finding entry
        # If finding was created by AnalysisEngine, it already has correct data
        # We just need to add it to our storage
        self._findings[finding.id] = finding
        self._dedup_index[dedup_key] = finding
        self._total_new += 1

        logger.debug(
            "finding_added",
            finding_id=str(finding.id),
            event_type=event_type,
            severity=finding.severity.value,
        )
        return finding

    def _within_window(self, last_seen: datetime, new_time: datetime) -> bool:
        """Check if new event is within clustering window of last seen.

        Args:
            last_seen: Timestamp of last occurrence
            new_time: Timestamp of new occurrence

        Returns:
            True if events should be clustered together
        """
        # Handle case where new_time is before last_seen (out of order events)
        time_diff = abs(new_time - last_seen)
        return time_diff <= self._cluster_window

    def get_finding(self, finding_id: UUID) -> Optional[Finding]:
        """Get a specific finding by ID.

        Args:
            finding_id: UUID of the finding

        Returns:
            Finding if found, None otherwise
        """
        return self._findings.get(finding_id)

    def get_all_findings(self) -> List[Finding]:
        """Get all stored findings.

        Returns:
            List of all Finding objects (deduplicated)
        """
        return list(self._findings.values())

    def get_findings_by_severity(self, severity) -> List[Finding]:
        """Get findings filtered by severity.

        Args:
            severity: Severity enum value to filter by

        Returns:
            List of findings with matching severity
        """
        return [f for f in self._findings.values() if f.severity == severity]

    def get_findings_by_category(self, category) -> List[Finding]:
        """Get findings filtered by category.

        Args:
            category: Category enum value to filter by

        Returns:
            List of findings with matching category
        """
        return [f for f in self._findings.values() if f.category == category]

    def get_recurring_findings(self) -> List[Finding]:
        """Get all findings marked as recurring.

        Returns:
            List of findings where is_recurring is True
        """
        return [f for f in self._findings.values() if f.is_recurring]

    def clear(self) -> None:
        """Clear all stored findings and reset statistics."""
        self._findings.clear()
        self._dedup_index.clear()
        self._total_merged = 0
        self._total_new = 0

    def get_summary(self) -> Dict[str, any]:
        """Get a summary of findings by severity and category.

        Returns:
            Dictionary with counts by severity and category
        """
        from unifi_scanner.models.enums import Severity, Category

        summary = {
            "total": len(self._findings),
            "by_severity": {s.value: 0 for s in Severity},
            "by_category": {c.value: 0 for c in Category},
            "recurring_count": 0,
            "total_occurrences": 0,
        }

        for finding in self._findings.values():
            summary["by_severity"][finding.severity.value] += 1
            summary["by_category"][finding.category.value] += 1
            summary["total_occurrences"] += finding.occurrence_count
            if finding.is_recurring:
                summary["recurring_count"] += 1

        return summary
```

2. Update `src/unifi_scanner/analysis/__init__.py` to export FindingStore:

```python
"""Analysis engine for UniFi log processing."""

from unifi_scanner.analysis.engine import AnalysisEngine
from unifi_scanner.analysis.rules import Rule, RuleRegistry
from unifi_scanner.analysis.store import FindingStore

__all__ = ["AnalysisEngine", "Rule", "RuleRegistry", "FindingStore"]
```
  </action>
  <verify>
Run: `python -c "from unifi_scanner.analysis import FindingStore; s = FindingStore(); print(f'Window: {s.cluster_window}, Stats: {s.stats}')"`
Should print: `Window: 1:00:00, Stats: {'total_findings': 0, 'total_merged': 0, 'total_new': 0}`
  </verify>
  <done>
FindingStore implements time-window deduplication with 1-hour default window, tracks merged vs new findings, and provides filtering by severity/category.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create comprehensive tests for FindingStore</name>
  <files>tests/test_finding_store.py</files>
  <action>
Create `tests/test_finding_store.py`:

```python
"""Tests for FindingStore deduplication logic."""

import pytest
from datetime import datetime, timedelta, timezone
from uuid import uuid4

from unifi_scanner.analysis import FindingStore
from unifi_scanner.models.finding import Finding
from unifi_scanner.models.enums import Category, Severity


def make_finding(
    severity=Severity.MEDIUM,
    category=Category.CONNECTIVITY,
    first_seen=None,
    last_seen=None,
) -> Finding:
    """Helper to create a Finding for testing."""
    now = datetime.now(timezone.utc)
    return Finding(
        severity=severity,
        category=category,
        title="Test Finding",
        description="Test description",
        first_seen=first_seen or now,
        last_seen=last_seen or now,
    )


class TestFindingStoreBasics:
    """Test basic store operations."""

    def test_add_new_finding(self):
        """Test adding a new finding."""
        store = FindingStore()
        finding = make_finding()
        log_id = uuid4()

        result = store.add_or_merge(
            finding,
            event_type="EVT_TEST",
            device_mac="aa:bb:cc:dd:ee:ff",
            timestamp=datetime.now(timezone.utc),
            log_id=log_id,
        )

        assert result.id == finding.id
        assert store.stats["total_new"] == 1
        assert store.stats["total_merged"] == 0
        assert len(store.get_all_findings()) == 1

    def test_get_finding_by_id(self):
        """Test retrieving a finding by ID."""
        store = FindingStore()
        finding = make_finding()

        store.add_or_merge(
            finding,
            event_type="EVT_TEST",
            device_mac=None,
            timestamp=datetime.now(timezone.utc),
            log_id=uuid4(),
        )

        retrieved = store.get_finding(finding.id)
        assert retrieved is not None
        assert retrieved.id == finding.id

    def test_clear_store(self):
        """Test clearing the store."""
        store = FindingStore()
        store.add_or_merge(
            make_finding(),
            event_type="EVT_TEST",
            device_mac=None,
            timestamp=datetime.now(timezone.utc),
            log_id=uuid4(),
        )

        store.clear()

        assert len(store.get_all_findings()) == 0
        assert store.stats["total_new"] == 0


class TestDeduplication:
    """Test time-window deduplication logic."""

    def test_merge_within_window(self):
        """Test that events within 1 hour are merged."""
        store = FindingStore()
        base_time = datetime(2024, 1, 15, 10, 0, 0, tzinfo=timezone.utc)

        # First occurrence
        finding1 = make_finding(first_seen=base_time, last_seen=base_time)
        store.add_or_merge(
            finding1,
            event_type="EVT_AP_Lost_Contact",
            device_mac="aa:bb:cc:dd:ee:ff",
            timestamp=base_time,
            log_id=uuid4(),
        )

        # Second occurrence 30 minutes later (within window)
        second_time = base_time + timedelta(minutes=30)
        finding2 = make_finding(first_seen=second_time, last_seen=second_time)
        result = store.add_or_merge(
            finding2,
            event_type="EVT_AP_Lost_Contact",
            device_mac="aa:bb:cc:dd:ee:ff",
            timestamp=second_time,
            log_id=uuid4(),
        )

        # Should be merged into first finding
        assert result.id == finding1.id
        assert result.occurrence_count == 2
        assert store.stats["total_new"] == 1
        assert store.stats["total_merged"] == 1

    def test_new_finding_outside_window(self):
        """Test that events outside 1 hour window create new finding."""
        store = FindingStore()
        base_time = datetime(2024, 1, 15, 10, 0, 0, tzinfo=timezone.utc)

        # First occurrence
        finding1 = make_finding(first_seen=base_time, last_seen=base_time)
        store.add_or_merge(
            finding1,
            event_type="EVT_AP_Lost_Contact",
            device_mac="aa:bb:cc:dd:ee:ff",
            timestamp=base_time,
            log_id=uuid4(),
        )

        # Second occurrence 2 hours later (outside window)
        second_time = base_time + timedelta(hours=2)
        finding2 = make_finding(first_seen=second_time, last_seen=second_time)
        result = store.add_or_merge(
            finding2,
            event_type="EVT_AP_Lost_Contact",
            device_mac="aa:bb:cc:dd:ee:ff",
            timestamp=second_time,
            log_id=uuid4(),
        )

        # Should be a new finding
        assert result.id == finding2.id
        assert result.occurrence_count == 1
        assert len(store.get_all_findings()) == 2
        assert store.stats["total_new"] == 2

    def test_different_device_mac_not_merged(self):
        """Test that same event on different devices are not merged."""
        store = FindingStore()
        now = datetime.now(timezone.utc)

        finding1 = make_finding(first_seen=now, last_seen=now)
        store.add_or_merge(
            finding1,
            event_type="EVT_AP_Lost_Contact",
            device_mac="aa:bb:cc:dd:ee:ff",
            timestamp=now,
            log_id=uuid4(),
        )

        finding2 = make_finding(first_seen=now, last_seen=now)
        result = store.add_or_merge(
            finding2,
            event_type="EVT_AP_Lost_Contact",
            device_mac="11:22:33:44:55:66",  # Different MAC
            timestamp=now,
            log_id=uuid4(),
        )

        # Should be separate findings
        assert result.id == finding2.id
        assert len(store.get_all_findings()) == 2

    def test_different_event_type_not_merged(self):
        """Test that different event types are not merged."""
        store = FindingStore()
        now = datetime.now(timezone.utc)

        finding1 = make_finding(first_seen=now, last_seen=now)
        store.add_or_merge(
            finding1,
            event_type="EVT_AP_Lost_Contact",
            device_mac="aa:bb:cc:dd:ee:ff",
            timestamp=now,
            log_id=uuid4(),
        )

        finding2 = make_finding(first_seen=now, last_seen=now)
        result = store.add_or_merge(
            finding2,
            event_type="EVT_AP_Disconnected",  # Different event
            device_mac="aa:bb:cc:dd:ee:ff",
            timestamp=now,
            log_id=uuid4(),
        )

        assert len(store.get_all_findings()) == 2

    def test_null_device_mac_deduplication(self):
        """Test deduplication works with None device_mac."""
        store = FindingStore()
        base_time = datetime.now(timezone.utc)

        finding1 = make_finding(first_seen=base_time, last_seen=base_time)
        store.add_or_merge(
            finding1,
            event_type="EVT_AD_LOGIN_FAILED",
            device_mac=None,
            timestamp=base_time,
            log_id=uuid4(),
        )

        # Same event, also no device MAC, within window
        second_time = base_time + timedelta(minutes=15)
        finding2 = make_finding(first_seen=second_time, last_seen=second_time)
        result = store.add_or_merge(
            finding2,
            event_type="EVT_AD_LOGIN_FAILED",
            device_mac=None,
            timestamp=second_time,
            log_id=uuid4(),
        )

        assert result.id == finding1.id
        assert result.occurrence_count == 2

    def test_custom_cluster_window(self):
        """Test using a custom time window."""
        store = FindingStore(cluster_window=timedelta(minutes=15))
        base_time = datetime(2024, 1, 15, 10, 0, 0, tzinfo=timezone.utc)

        finding1 = make_finding(first_seen=base_time, last_seen=base_time)
        store.add_or_merge(
            finding1,
            event_type="EVT_TEST",
            device_mac=None,
            timestamp=base_time,
            log_id=uuid4(),
        )

        # 20 minutes later - outside 15 minute window
        second_time = base_time + timedelta(minutes=20)
        finding2 = make_finding(first_seen=second_time, last_seen=second_time)
        store.add_or_merge(
            finding2,
            event_type="EVT_TEST",
            device_mac=None,
            timestamp=second_time,
            log_id=uuid4(),
        )

        assert len(store.get_all_findings()) == 2


class TestRecurringIssues:
    """Test recurring issue detection."""

    def test_recurring_threshold(self):
        """Test that 5+ occurrences flags as recurring."""
        store = FindingStore()
        base_time = datetime.now(timezone.utc)

        # Create finding with multiple occurrences
        finding = make_finding(first_seen=base_time, last_seen=base_time)

        # Add first occurrence
        store.add_or_merge(
            finding,
            event_type="EVT_TEST",
            device_mac=None,
            timestamp=base_time,
            log_id=uuid4(),
        )

        # Add 4 more occurrences (within window)
        for i in range(4):
            store.add_or_merge(
                finding,  # Same finding gets reused in merge
                event_type="EVT_TEST",
                device_mac=None,
                timestamp=base_time + timedelta(minutes=i + 1),
                log_id=uuid4(),
            )

        result = store.get_all_findings()[0]
        assert result.occurrence_count == 5
        assert result.is_recurring is True

    def test_not_recurring_below_threshold(self):
        """Test that 4 occurrences is not recurring."""
        store = FindingStore()
        base_time = datetime.now(timezone.utc)

        finding = make_finding(first_seen=base_time, last_seen=base_time)
        store.add_or_merge(
            finding,
            event_type="EVT_TEST",
            device_mac=None,
            timestamp=base_time,
            log_id=uuid4(),
        )

        # Add 3 more (total 4)
        for i in range(3):
            store.add_or_merge(
                finding,
                event_type="EVT_TEST",
                device_mac=None,
                timestamp=base_time + timedelta(minutes=i + 1),
                log_id=uuid4(),
            )

        result = store.get_all_findings()[0]
        assert result.occurrence_count == 4
        assert result.is_recurring is False

    def test_get_recurring_findings(self):
        """Test filtering for recurring findings only."""
        store = FindingStore()
        base_time = datetime.now(timezone.utc)

        # Add recurring finding (5 occurrences)
        finding1 = make_finding(first_seen=base_time, last_seen=base_time)
        for i in range(5):
            store.add_or_merge(
                finding1,
                event_type="EVT_RECURRING",
                device_mac=None,
                timestamp=base_time + timedelta(minutes=i),
                log_id=uuid4(),
            )

        # Add non-recurring finding (1 occurrence)
        finding2 = make_finding(first_seen=base_time, last_seen=base_time)
        store.add_or_merge(
            finding2,
            event_type="EVT_SINGLE",
            device_mac=None,
            timestamp=base_time,
            log_id=uuid4(),
        )

        recurring = store.get_recurring_findings()
        assert len(recurring) == 1
        assert recurring[0].occurrence_count == 5


class TestFiltering:
    """Test filtering methods."""

    def test_filter_by_severity(self):
        """Test filtering findings by severity."""
        store = FindingStore()
        now = datetime.now(timezone.utc)

        # Add SEVERE finding
        severe_finding = make_finding(severity=Severity.SEVERE)
        store.add_or_merge(
            severe_finding,
            event_type="EVT_SEVERE",
            device_mac=None,
            timestamp=now,
            log_id=uuid4(),
        )

        # Add LOW finding
        low_finding = make_finding(severity=Severity.LOW)
        store.add_or_merge(
            low_finding,
            event_type="EVT_LOW",
            device_mac=None,
            timestamp=now,
            log_id=uuid4(),
        )

        severe = store.get_findings_by_severity(Severity.SEVERE)
        assert len(severe) == 1
        assert severe[0].severity == Severity.SEVERE

    def test_filter_by_category(self):
        """Test filtering findings by category."""
        store = FindingStore()
        now = datetime.now(timezone.utc)

        # Add SECURITY finding
        security_finding = make_finding(category=Category.SECURITY)
        store.add_or_merge(
            security_finding,
            event_type="EVT_SECURITY",
            device_mac=None,
            timestamp=now,
            log_id=uuid4(),
        )

        # Add SYSTEM finding
        system_finding = make_finding(category=Category.SYSTEM)
        store.add_or_merge(
            system_finding,
            event_type="EVT_SYSTEM",
            device_mac=None,
            timestamp=now,
            log_id=uuid4(),
        )

        security = store.get_findings_by_category(Category.SECURITY)
        assert len(security) == 1
        assert security[0].category == Category.SECURITY


class TestSummary:
    """Test summary generation."""

    def test_get_summary(self):
        """Test summary statistics."""
        store = FindingStore()
        now = datetime.now(timezone.utc)

        # Add various findings
        store.add_or_merge(
            make_finding(severity=Severity.SEVERE, category=Category.SECURITY),
            "EVT_1", None, now, uuid4()
        )
        store.add_or_merge(
            make_finding(severity=Severity.MEDIUM, category=Category.PERFORMANCE),
            "EVT_2", None, now, uuid4()
        )
        store.add_or_merge(
            make_finding(severity=Severity.LOW, category=Category.SYSTEM),
            "EVT_3", None, now, uuid4()
        )

        summary = store.get_summary()

        assert summary["total"] == 3
        assert summary["by_severity"]["severe"] == 1
        assert summary["by_severity"]["medium"] == 1
        assert summary["by_severity"]["low"] == 1
        assert summary["by_category"]["security"] == 1


class TestOccurrenceSummaryFormat:
    """Test the occurrence summary formatting."""

    def test_single_occurrence_format(self):
        """Test format for single occurrence."""
        finding = make_finding(
            first_seen=datetime(2024, 1, 15, 14, 0, 0, tzinfo=timezone.utc)
        )
        finding.last_seen = finding.first_seen

        summary = finding.format_occurrence_summary()
        assert "Occurred at" in summary
        assert "02:00 PM" in summary

    def test_multiple_occurrence_format(self):
        """Test format for multiple occurrences."""
        finding = make_finding(
            first_seen=datetime(2024, 1, 15, 14, 0, 0, tzinfo=timezone.utc),
            last_seen=datetime(2024, 1, 15, 16, 30, 0, tzinfo=timezone.utc),
        )
        finding.occurrence_count = 5

        summary = finding.format_occurrence_summary()
        assert "Occurred 5 times" in summary
        assert "first: 02:00 PM" in summary
        assert "last: 04:30 PM" in summary

    def test_recurring_label_in_summary(self):
        """Test that recurring issues show label."""
        finding = make_finding(
            first_seen=datetime.now(timezone.utc)
        )
        finding.last_seen = finding.first_seen
        finding.occurrence_count = 10  # Above threshold

        summary = finding.format_occurrence_summary()
        assert "[Recurring Issue]" in summary
```
  </action>
  <verify>
Run: `pytest tests/test_finding_store.py -v`
All tests should pass.
  </verify>
  <done>
Comprehensive tests cover deduplication within/outside time window, device MAC handling, recurring detection, filtering by severity/category, and summary formatting.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from unifi_scanner.models import Finding; print(Finding.RECURRING_THRESHOLD)"` - should print 5
2. `python -c "from unifi_scanner.analysis import FindingStore; print('FindingStore imported')"`
3. `pytest tests/test_finding_store.py -v` - all tests pass
4. Integration test:
   ```python
   from datetime import datetime, timedelta, timezone
   from uuid import uuid4
   from unifi_scanner.analysis import FindingStore
   from unifi_scanner.models.finding import Finding
   from unifi_scanner.models.enums import Category, Severity

   store = FindingStore()
   base = datetime.now(timezone.utc)

   # Add 3 occurrences within window
   for i in range(3):
       f = Finding(severity=Severity.MEDIUM, category=Category.CONNECTIVITY,
                   title="Test", description="Test",
                   first_seen=base, last_seen=base)
       store.add_or_merge(f, "EVT_TEST", "aa:bb:cc:dd:ee:ff",
                         base + timedelta(minutes=i*10), uuid4())

   print(f"Findings: {len(store.get_all_findings())}, Merged: {store.stats['total_merged']}")
   # Should print: Findings: 1, Merged: 2
   ```
</verification>

<success_criteria>
- Finding.RECURRING_THRESHOLD = 5 (configurable at class level)
- Finding.is_recurring returns True when occurrence_count >= 5
- Finding.format_occurrence_summary() returns user-specified format
- FindingStore.cluster_window defaults to 1 hour
- add_or_merge() merges findings within time window
- add_or_merge() creates new findings outside time window
- Deduplication key is (event_type, device_mac)
- None device_mac is valid deduplication key
- get_findings_by_severity() and get_findings_by_category() filter correctly
- get_recurring_findings() returns only findings with 5+ occurrences
- get_summary() provides counts by severity and category
- Tests cover all deduplication scenarios
</success_criteria>

<output>
After completion, create `.planning/phases/03-analysis-engine/03-03-SUMMARY.md`
</output>
